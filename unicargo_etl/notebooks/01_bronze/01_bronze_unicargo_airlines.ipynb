{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379ebb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tables config from: C:\\Users\\Dele\\Documents\\D. Professional Registration\\IT\\DATA-EnGR\\00_data_engr_projects\\unicargo\\unicargo_dab\\tables.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "# Current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Go up 3 levels and append 'src'\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..', 'src'))\n",
    "# Add src to sys.path\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from logging_utils import log_task_status, TaskLogger\n",
    "from unikargo_utils import add_pipeline_metadata\n",
    "from config import get_log_config\n",
    "\n",
    "from io_utils import write_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba6dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dele\\Documents\\D. Professional Registration\\IT\\DATA-EnGR\\00_data_engr_projects\\unicargo\\unicargo_dab\\.venv_dbc\\Lib\\site-packages\\databricks\\sdk\\_widgets\\__init__.py:71: UserWarning: \n",
      "To use databricks widgets interactively in your notebook, please install databricks sdk using:\n",
      "\tpip install 'databricks-sdk[notebook]'\n",
      "Falling back to default_value_only implementation for databricks widgets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create widgets (required for ADF → Databricks integration)\n",
    "dbutils.widgets.text(\"pipeline_id\", \"\")\n",
    "dbutils.widgets.text(\"run_id\", \"\")\n",
    "dbutils.widgets.text(\"task_id\", \"\")\n",
    "dbutils.widgets.text(\"processed_timestamp\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"unikargo_dev\") # Requires an ADF variable for ADF runs\n",
    "\n",
    "# # Extract values from widgets\n",
    "pipeline_id = dbutils.widgets.get(\"pipeline_id\")\n",
    "run_id = dbutils.widgets.get(\"run_id\")\n",
    "task_id = dbutils.widgets.get(\"task_id\")\n",
    "processed_timestamp = dbutils.widgets.get(\"processed_timestamp\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "\n",
    "# Logging parameters for run context\n",
    "pipeline_name = \"airlines_ingestion\"\n",
    "environment = \"dev\"\n",
    "# run_id = str(uuid.uuid4())\n",
    "start_time = datetime.now()\n",
    "# status = \"SUCCESS\"\n",
    "# message = \"\"\n",
    "rows_processed = 0\n",
    "\n",
    "\n",
    "# target_path=target_path\n",
    "\n",
    "log_type =  'task'\n",
    "environment = 'dev'\n",
    "\n",
    "source_path=\"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv\"\n",
    "target_path = get_log_config(log_type, environment=environment)\n",
    "\n",
    "# Path to save logging for tasks\n",
    "# LOG_PATH_TASK = \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/task_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03562fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# airline schema\n",
    "airlines_schema = StructType([\n",
    "    StructField(\"iata_code\", StringType(), True),\n",
    "    StructField(\"airline\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e42ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Read Airlines Data ---\n",
    "with TaskLogger(\n",
    "    \"read_airlines_csv\",\n",
    "    pipeline_name=pipeline_name,\n",
    "    source_path=source_path,\n",
    "    target_path=target_path,\n",
    "    log_running=False  # keep this False unless you explicitly want a \"RUNNING\" entry\n",
    ") as logger:\n",
    "    \n",
    "    airlines_df = (\n",
    "        spark.read\n",
    "        .schema(airlines_schema)    \n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(source_path)\n",
    "    )\n",
    "    \n",
    "    rows_processed = airlines_df.count()\n",
    "    \n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4312f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Read\n",
    "try:\n",
    "    # -----------------------------\n",
    "    # 1️Read the airlines CSV\n",
    "    # -----------------------------\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    airlines_df = (\n",
    "        spark.read\n",
    "        .schema(airlines_schema)    \n",
    "        .option(\"header\", \"true\")   # use the header row for column names only\n",
    "        .csv(source_path) # added for adf\n",
    "        )\n",
    "    \n",
    "    rows_processed = airlines_df.count()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Log SUCCESS to task logs\n",
    "    # -----------------------------\n",
    "    log_task_status(\n",
    "        status=\"SUCCESS\",\n",
    "        operation=\"read_airlines_csv\",\n",
    "        rows=rows_processed,\n",
    "        start_time=start_time,\n",
    "        pipeline_name=pipeline_name,\n",
    "        source_path=source_path,\n",
    "        target_path=target_path,\n",
    "        pipeline_id=None\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    # -----------------------------\n",
    "    # Log FAILURE to task logs\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            operation=\"read_airlines_csv\",  # Same operation name\n",
    "            error=e,\n",
    "            start_time=start_time,\n",
    "            pipeline_name=pipeline_name,\n",
    "            source_path=source_path,\n",
    "            pipeline_id=None\n",
    "        )\n",
    "        \n",
    "    except Exception as log_error:\n",
    "        print(f\"Failed to write task log: {log_error}\")\n",
    "    \n",
    "    # Re-raise original error for debugging\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9279cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 2: Add metadata to the dataframe (Pipeline identifier, Run identifier and Task identifier)\n",
    "try:\n",
    "    airlines_df = add_pipeline_metadata(airlines_df, pipeline_id, run_id, task_id)\n",
    "    \n",
    "    # Count rows after transformation\n",
    "    rows_processed = airlines_df.count()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Log SUCCESS to task logs\n",
    "    # -----------------------------\n",
    "    log_task_status(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Metadata column added successfully\",\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_id=None,\n",
    "        file_format=\"delta\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    # -----------------------------\n",
    "    # Log FAILURE to task logs\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e),\n",
    "            pipeline_name=pipeline_name,\n",
    "            pipeline_id=None,\n",
    "            file_format=\"delta\"\n",
    "        )\n",
    "    except Exception as log_error:\n",
    "        print(f\"Failed to write task log: {log_error}\")\n",
    "    \n",
    "    # Re-raise original error for debugging\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c275e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 3: Write to bronze\n",
    "\n",
    "try:\n",
    "    # Count rows first\n",
    "    rows_processed = airlines_df.count()\n",
    "\n",
    "    # Overwrite Delta table safely\n",
    "    airlines_df.write.\\\n",
    "    mode(\"overwrite\").\\\n",
    "    option(\"overwriteSchema\", \"true\").\\\n",
    "    saveAsTable(f\"`{catalog}`.`01_bronze`.`unikargo_airlines_bronze`\")\n",
    "\n",
    "    log_task_status(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Airlines data written successfully\",\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_id=None,\n",
    "        file_format=\"delta\"\n",
    "    )\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    # Log FAILURE safely\n",
    "    try:\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e)\n",
    "        )\n",
    "    except AnalysisException as log_e:\n",
    "        print(f\"Failed to log task event: {log_e}\")\n",
    "    raise  # re-raise original error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Log read task for debugging\n",
    "# try:\n",
    "#     logs_df = spark.read.format(\"delta\").load(LOG_PATH_TASK)\n",
    "#     logs_df.orderBy(col(\"timestamp\").desc()).show(10, truncate=False)\n",
    "# except Exception:\n",
    "#     print(f\"No task logs found yet at {LOG_PATH_TASK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf52d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------+-----------+--------------------+----------+-----------------+-------+----+-----------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------+----------+-------------+--------------------------+----------+\n",
      "|pipeline_id                         |pipeline_name     |environment|run_id              |task_id   |operation        |status |rows|execution_time_ms|source_path                                                                     |target_path                                                               |error_type|error_message|timestamp                 |log_date  |\n",
      "+------------------------------------+------------------+-----------+--------------------+----------+-----------------+-------+----+-----------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------+----------+-------------+--------------------------+----------+\n",
      "|3f695ea3-f3cf-4212-96a3-2809574a098a|airlines_ingestion|dev        |local_run_1757124139|local_task|read_airlines_csv|SUCCESS|14  |749              |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 02:02:20.679745|2025-09-06|\n",
      "|086ecd02-ff80-4978-87de-52a2068d5a69|airlines_ingestion|dev        |local_run_1757124112|local_task|read_airlines_csv|SUCCESS|14  |705              |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 02:01:54.24604 |2025-09-06|\n",
      "|7ece05a6-7d93-4682-9aa8-b0923a25f511|airlines_ingestion|dev        |local_run_1757124108|local_task|read_airlines_csv|SUCCESS|14  |631              |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 02:01:49.397506|2025-09-06|\n",
      "|b62629d7-ac5b-4c94-9813-99cd066b37cb|airlines_ingestion|dev        |local_run_1757124078|local_task|read_airlines_csv|SUCCESS|14  |1347             |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 02:01:19.840765|2025-09-06|\n",
      "|41a6aece-09d0-46bf-bef6-64a12c7b577f|airlines_ingestion|dev        |local_run_1757123432|local_task|read_airlines_csv|SUCCESS|14  |770              |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 01:50:33.688914|2025-09-06|\n",
      "|bbab1f77-e67c-4ecb-906f-f114d4d3b00a|airlines_ingestion|dev        |local_run_1757123408|local_task|read_airlines_csv|SUCCESS|14  |1761             |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 01:50:09.392581|2025-09-06|\n",
      "|1c587f88-535f-4e7b-b71c-82358d796659|airlines_ingestion|dev        |local_run_1757123406|local_task|read_airlines_csv|RUNNING|NULL|0                |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 01:50:07.712017|2025-09-06|\n",
      "|4a07e12d-f413-49b6-84de-1dc0f7a192f7|airlines_ingestion|dev        |local_run_1757123369|local_task|read_airlines_csv|SUCCESS|14  |15742            |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 01:49:30.824518|2025-09-06|\n",
      "|0f8783bd-814a-4abb-9941-9bbf0d2e7e40|airlines_ingestion|dev        |local_run_1757123353|local_task|read_airlines_csv|RUNNING|NULL|0                |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 01:49:24.687696|2025-09-06|\n",
      "|2067da76-3d7b-485b-9f8c-5e603f546af7|airlines_ingestion|dev        |local_run_1757120269|local_task|read_airlines_csv|SUCCESS|14  |1716             |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:57:50.412603|2025-09-06|\n",
      "|5712d2db-b3ca-4b3c-aca2-0892bdbf0733|airlines_ingestion|dev        |local_run_1757120267|local_task|read_airlines_csv|RUNNING|NULL|0                |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:57:48.797688|2025-09-06|\n",
      "|b9809063-7f9f-4556-8e5d-5b6e64ec86e1|airlines_ingestion|dev        |local_run_1757120233|local_task|read_airlines_csv|SUCCESS|14  |1672             |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:57:14.403918|2025-09-06|\n",
      "|a3ad0f69-39bf-4d15-8ad9-6b14515627c4|airlines_ingestion|dev        |local_run_1757120231|local_task|read_airlines_csv|RUNNING|NULL|0                |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:57:12.634085|2025-09-06|\n",
      "|e57ebd13-0262-46ba-ad97-5fc7982df638|airlines_ingestion|dev        |local_run_1757120223|local_task|read_airlines_csv|SUCCESS|14  |1953             |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:57:04.284932|2025-09-06|\n",
      "|f654efc1-95a1-4229-837b-ed1fa311257f|airlines_ingestion|dev        |local_run_1757120221|local_task|read_airlines_csv|RUNNING|NULL|0                |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:57:02.383118|2025-09-06|\n",
      "|7a9a297f-8ea1-4152-a64d-b6a44cf59970|airlines_ingestion|dev        |local_run_1757120080|local_task|read_airlines_csv|SUCCESS|14  |2411             |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:54:41.852043|2025-09-06|\n",
      "|837b3a04-3a2a-472e-b818-68654320d950|airlines_ingestion|dev        |local_run_1757120078|local_task|read_airlines_csv|RUNNING|NULL|0                |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:54:39.947107|2025-09-06|\n",
      "|c31cb064-80bd-4b1e-8a5a-32e2f3eeca56|airlines_ingestion|dev        |local_run_1757119486|local_task|read_airlines_csv|SUCCESS|14  |767              |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:44:47.912843|2025-09-06|\n",
      "|d053854b-6911-427f-ba02-ada8ce39f701|airlines_ingestion|dev        |local_run_1757119467|local_task|read_airlines_csv|SUCCESS|14  |2069             |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:44:28.302911|2025-09-06|\n",
      "|871df3a9-241e-4d9f-83bc-360a29d6f56a|airlines_ingestion|dev        |local_run_1757119465|local_task|read_airlines_csv|RUNNING|NULL|0                |abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv|abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs|NULL      |NULL         |2025-09-06 00:44:26.584641|2025-09-06|\n",
      "+------------------------------------+------------------+-----------+--------------------+----------+-----------------+-------+----+-----------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------+----------+-------------+--------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set environment\n",
    "environment = \"dev\"  # dev / staging / prod\n",
    "\n",
    "# Get the log path from your config\n",
    "log_base_path = {\n",
    "    \"dev\": \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/dev_task_logs\",\n",
    "    \"staging\": \"abfss://medallion@adlsunikargostg.dfs.core.windows.net/logs/staging_task_logs\",\n",
    "    \"prod\": \"abfss://medallion@adlsunikargoprd.dfs.core.windows.net/logs/prod_task_logs\"\n",
    "}[environment]\n",
    "\n",
    "# Read Delta logs\n",
    "logs_df = spark.read.format(\"delta\").load(log_base_path)\n",
    "\n",
    "# Show latest logs\n",
    "logs_df.orderBy(\"timestamp\", ascending=False).show(20, truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef42c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Read (with simulated failure)\n",
    "try:\n",
    "    # Simulate a failure by pointing to a non-existent file\n",
    "    airlines_df = (\n",
    "        spark.read\n",
    "        .schema(airlines_schema)\n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(\"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/non_existent_file.csv\")\n",
    "    )\n",
    "\n",
    "    rows_processed = airlines_df.count()\n",
    "\n",
    "    # Log SUCCESS\n",
    "    log_task_status(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Airlines data read successfully\",\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_id=None,\n",
    "        file_format=\"delta\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    # Log FAILURE\n",
    "    try:\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e),\n",
    "            pipeline_name=pipeline_name,\n",
    "            pipeline_id=None,\n",
    "            file_format=\"delta\"\n",
    "        )\n",
    "    except Exception as log_error:\n",
    "        print(f\"Failed to write task log: {log_error}\")\n",
    "\n",
    "    print(\"Simulated failure caught. Task log written.\")\n",
    "    # Optionally continue without raising\n",
    "    # raise  # Uncomment to still crash the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Read (with simulated failure)\n",
    "\n",
    "tasks = [\n",
    "    {\"name\": \"read_airlines\", \"path\": \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv\"},\n",
    "    {\"name\": \"read_invalid\", \"path\": \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/missing_file.csv\"},\n",
    "    {\"name\": \"read_flights\", \"path\": \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/flights.csv\"}\n",
    "]\n",
    "\n",
    "for task in tasks:\n",
    "    try:\n",
    "        print(f\"Running task: {task['name']}\")\n",
    "        \n",
    "        # Attempt to read CSV\n",
    "        df = (\n",
    "            spark.read\n",
    "            .schema(airlines_schema)  # or flights_schema for other tasks\n",
    "            .option(\"header\", \"true\")\n",
    "            .csv(task[\"path\"])\n",
    "        )\n",
    "        rows_processed = df.count()\n",
    "        \n",
    "        # Log success\n",
    "        log_task_status(\n",
    "            status=\"SUCCESS\",\n",
    "            rows=rows_processed,\n",
    "            message=f\"{task['name']} completed successfully\",\n",
    "            pipeline_name=pipeline_name,\n",
    "            pipeline_id=None,\n",
    "            file_format=\"delta\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log failure but continue\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e),\n",
    "            pipeline_name=pipeline_name,\n",
    "            pipeline_id=None,\n",
    "            file_format=\"delta\"\n",
    "        )\n",
    "        print(f\"Task {task['name']} failed: {e}\")\n",
    "        # Do NOT raise, continue to next task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    {\"name\": \"read_airlines\", \"path\": \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv\", \"fail_transform\": False},\n",
    "    {\"name\": \"read_flights\", \"path\": \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/flights.csv\", \"fail_transform\": True}\n",
    "]\n",
    "\n",
    "for task in tasks:\n",
    "    try:\n",
    "        print(f\"Running task: {task['name']}\")\n",
    "        \n",
    "        # Read CSV\n",
    "        df = (\n",
    "            spark.read\n",
    "            .schema(airlines_schema)  # or flights_schema for flights\n",
    "            .option(\"header\", \"true\")\n",
    "            .csv(task[\"path\"])\n",
    "        )\n",
    "        \n",
    "        # Simulate a runtime error during transformation\n",
    "        if task.get(\"fail_transform\"):\n",
    "            # Example: divide by zero or invalid operation\n",
    "            df = df.withColumn(\"simulate_error\", df[\"iata_code\"] / 0)\n",
    "        \n",
    "        rows_processed = df.count()\n",
    "        \n",
    "        # Log success\n",
    "        log_task_status(\n",
    "            status=\"SUCCESS\",\n",
    "            rows=rows_processed,\n",
    "            message=f\"{task['name']} completed successfully\",\n",
    "            pipeline_name=pipeline_name,\n",
    "            pipeline_id=None,\n",
    "            file_format=\"delta\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log failure but continue\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e),\n",
    "            pipeline_name=pipeline_name,\n",
    "            pipeline_id=None,\n",
    "            file_format=\"delta\"\n",
    "        )\n",
    "        print(f\"Task {task['name']} failed: {e}\")\n",
    "        # Continue to next task without stopping notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ca87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    {\"name\": \"read_airlines\", \"path\": \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv\", \"fail_transform\": False},\n",
    "    {\"name\": \"read_flights\", \"path\": \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/flights.csv\", \"fail_transform\": True}\n",
    "]\n",
    "\n",
    "for task in tasks:\n",
    "    try:\n",
    "        print(f\"Running task: {task['name']}\")\n",
    "        \n",
    "        # Read CSV\n",
    "        df = (\n",
    "            spark.read\n",
    "            .schema(airlines_schema)  # adjust schema per dataset\n",
    "            .option(\"header\", \"true\")\n",
    "            .csv(task[\"path\"])\n",
    "        )\n",
    "        \n",
    "        # Simulate a runtime error during transformation\n",
    "        if task.get(\"fail_transform\"):\n",
    "            df = df.withColumn(\"simulate_error\", df[\"iata_code\"] / 0)\n",
    "        \n",
    "        rows_processed = df.count()\n",
    "        \n",
    "        # Log success\n",
    "        log_task_status(\n",
    "            status=\"SUCCESS\",\n",
    "            rows=rows_processed,\n",
    "            message=f\"{task['name']} completed successfully\",\n",
    "            pipeline_name=pipeline_name,\n",
    "            pipeline_id=None,\n",
    "            file_format=\"delta\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log failure with rows_processed = 0\n",
    "        try:\n",
    "            log_task_status(\n",
    "                status=\"FAILED\",\n",
    "                rows=0,  # set to 0 on failure\n",
    "                message=str(e),\n",
    "                pipeline_name=pipeline_name,\n",
    "                pipeline_id=None,\n",
    "                file_format=\"delta\"\n",
    "            )\n",
    "        except Exception as log_error:\n",
    "            print(f\"Failed to write task log: {log_error}\")\n",
    "        \n",
    "        print(f\"Task {task['name']} failed: {e}\")\n",
    "        # Continue to next task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dbc (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
