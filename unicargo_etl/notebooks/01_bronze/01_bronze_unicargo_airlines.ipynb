{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "379ebb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType\n",
    "from pyspark.sql.functions import create_map, lit, current_timestamp\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b218e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Go up 3 levels and append 'src'\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..', 'src'))\n",
    "# Add src to sys.path\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from logging_utils.logger import log_pipeline_event, log_task_event\n",
    "from unikargo_utils import add_pipeline_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03562fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_schema = StructType([\n",
    "    StructField(\"iata_code\", StringType(), True),\n",
    "    StructField(\"airline\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ba6dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create widgets (required for ADF â†’ Databricks integration)\n",
    "dbutils.widgets.text(\"pipeline_id\", \"\")\n",
    "dbutils.widgets.text(\"run_id\", \"\")\n",
    "dbutils.widgets.text(\"task_id\", \"\")\n",
    "dbutils.widgets.text(\"processed_timestamp\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"unikargo_dev\") # Requires an ADF variable for ADF runs\n",
    "\n",
    "# # Extract values\n",
    "pipeline_id = dbutils.widgets.get(\"pipeline_id\")\n",
    "run_id = dbutils.widgets.get(\"run_id\")\n",
    "task_id = dbutils.widgets.get(\"task_id\")\n",
    "processed_timestamp = dbutils.widgets.get(\"processed_timestamp\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a387f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging parameters for run context\n",
    "\n",
    "pipeline_name = \"airlines_ingestion\"\n",
    "environment = \"dev\"\n",
    "# run_id = str(uuid.uuid4())\n",
    "\n",
    "start_time = datetime.now()\n",
    "status = \"SUCCESS\"\n",
    "message = \"\"\n",
    "rows_processed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4312f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Read\n",
    "try:\n",
    "    airlines_df = (\n",
    "        spark.read\n",
    "        .schema(airlines_schema)    # predefined schema\n",
    "        .option(\"header\", \"true\")   # use the header row for column names only\n",
    "        .csv(\"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airlines.csv\") # added for adf\n",
    "        )\n",
    "    rows_processed = airlines_df.count()\n",
    "    \n",
    "    # Log SUCCESS\n",
    "    log_task_event(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Airlines data read successfully\",\n",
    "        pipeline_name=pipeline_name\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    # --- Task 3: Log FAILURE\n",
    "    try:\n",
    "        log_task_event(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e)\n",
    "        )\n",
    "    except AnalysisException as log_e:\n",
    "        print(f\"Failed to log task event: {log_e}\")\n",
    "    raise  # re-raise original error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3a3a6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "|pipeline_id                         |pipeline_name       |environment |run_id               |task_id    |status |rows|message                           |timestamp                 |\n",
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "|7c95a047-5673-42e1-95f3-f3eb01fbaab7|airlines_ingestion  |unikargo_dev|manual_run_1756729892|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:31:32.996132|\n",
      "|dafb2265-e81a-49ee-a42e-412d67fcbe71|manual_pipeline_name|unikargo_dev|manual_run_1756729773|manual_task|SUCCESS|14  |Airlines data written successfully|2025-09-01 12:29:34.483091|\n",
      "|ffcb778e-d60e-4b72-a6cb-9bb7c5683208|manual_pipeline_name|unikargo_dev|manual_run_1756729770|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:29:30.798302|\n",
      "|f073dc07-9556-4fda-b7ce-8b30cf1499f8|airlines_ingestion  |unikargo_dev|manual_run_1756729767|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:29:28.170499|\n",
      "|4293daf6-c637-479a-a252-4ea5f2038f09|manual_pipeline_name|unikargo_dev|manual_run_1756729720|manual_task|SUCCESS|14  |Airlines data written successfully|2025-09-01 12:28:41.695016|\n",
      "|4c943969-71eb-4bcd-8a5f-299c40d45957|manual_pipeline_name|unikargo_dev|manual_run_1756729562|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:26:03.405529|\n",
      "|e394f5e1-560e-4ba5-a2ab-da08553e4da9|airlines_ingestion  |unikargo_dev|manual_run_1756729559|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:26:00.392464|\n",
      "|20ec3e4f-8438-4dbe-ae89-eece3d65046a|manual_pipeline_name|unikargo_dev|manual_run_1756729423|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:23:43.839565|\n",
      "|402a4c66-b755-445d-85a1-684a392839b8|airlines_ingestion  |unikargo_dev|manual_run_1756729420|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:23:41.194447|\n",
      "|local_test                          |manual_pipeline_name|unikargo_dev|manual_run_1756728979|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:16:20.524263|\n",
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "LOG_PATH_TASK = \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/task_logs\"\n",
    "\n",
    "try:\n",
    "    logs_df = spark.read.format(\"delta\").load(LOG_PATH_TASK)\n",
    "    logs_df.orderBy(col(\"timestamp\").desc()).show(10, truncate=False)\n",
    "except Exception:\n",
    "    print(f\"No task logs found yet at {LOG_PATH_TASK}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9279cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 2: Transform ---\n",
    "   # Add metadata to the dataframe (Pipeline identifier, Run identifier and Task identifier)\n",
    "try:\n",
    "    # df = df.withColumn(\n",
    "    #     \"metadata\",\n",
    "    #     create_map(\n",
    "    #         lit(\"pipeline_id\"), lit(pipeline_id),\n",
    "    #         lit(\"run_id\"), lit(run_id),\n",
    "    #         lit(\"task_id\"), lit(task_id),\n",
    "    #         # lit(\"processed_timestamp\"), lit(processed_timestamp),\n",
    "    #         lit(\"processed_timestamp\"), current_timestamp(),\n",
    "    #     ))\n",
    "\n",
    "  \n",
    "    airlines_df = add_pipeline_metadata(airlines_df, pipeline_id, run_id, task_id)\n",
    "    \n",
    "    # Count rows after transformation\n",
    "    rows_processed = airlines_df.count()\n",
    "\n",
    "    # Log SUCCESS\n",
    "    log_task_event(\n",
    "    status=\"SUCCESS\",\n",
    "    rows=rows_processed,\n",
    "    message=\"Metadata column added successfully\",\n",
    "    pipeline_name=pipeline_name\n",
    "\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log FAILURE\n",
    "    log_task_event(\n",
    "        status=\"FAILED\",\n",
    "        message=str(e),\n",
    "        pipeline_name=pipeline_id,\n",
    "  \n",
    "    )\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b726b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "|pipeline_id                         |pipeline_name       |environment |run_id               |task_id    |status |rows|message                           |timestamp                 |\n",
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "|1fe5a411-3ece-4011-bc5f-17b37db9d50a|airlines_ingestion  |unikargo_dev|manual_run_1756729894|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:31:35.29408 |\n",
      "|7c95a047-5673-42e1-95f3-f3eb01fbaab7|airlines_ingestion  |unikargo_dev|manual_run_1756729892|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:31:32.996132|\n",
      "|dafb2265-e81a-49ee-a42e-412d67fcbe71|manual_pipeline_name|unikargo_dev|manual_run_1756729773|manual_task|SUCCESS|14  |Airlines data written successfully|2025-09-01 12:29:34.483091|\n",
      "|ffcb778e-d60e-4b72-a6cb-9bb7c5683208|manual_pipeline_name|unikargo_dev|manual_run_1756729770|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:29:30.798302|\n",
      "|f073dc07-9556-4fda-b7ce-8b30cf1499f8|airlines_ingestion  |unikargo_dev|manual_run_1756729767|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:29:28.170499|\n",
      "|4293daf6-c637-479a-a252-4ea5f2038f09|manual_pipeline_name|unikargo_dev|manual_run_1756729720|manual_task|SUCCESS|14  |Airlines data written successfully|2025-09-01 12:28:41.695016|\n",
      "|4c943969-71eb-4bcd-8a5f-299c40d45957|manual_pipeline_name|unikargo_dev|manual_run_1756729562|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:26:03.405529|\n",
      "|e394f5e1-560e-4ba5-a2ab-da08553e4da9|airlines_ingestion  |unikargo_dev|manual_run_1756729559|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:26:00.392464|\n",
      "|20ec3e4f-8438-4dbe-ae89-eece3d65046a|manual_pipeline_name|unikargo_dev|manual_run_1756729423|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:23:43.839565|\n",
      "|402a4c66-b755-445d-85a1-684a392839b8|airlines_ingestion  |unikargo_dev|manual_run_1756729420|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:23:41.194447|\n",
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "LOG_PATH_TASK = \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/task_logs\"\n",
    "\n",
    "try:\n",
    "    logs_df = spark.read.format(\"delta\").load(LOG_PATH_TASK)\n",
    "    logs_df.orderBy(col(\"timestamp\").desc()).show(10, truncate=False)\n",
    "except Exception:\n",
    "    print(f\"No task logs found yet at {LOG_PATH_TASK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92aec6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Task 2: Transform\n",
    "# airlines_df = airlines_df.withColumn(\"metadata\",\n",
    "#                    create_map(\n",
    "#                        lit(\"pipeline_id\"), lit(pipeline_id),\n",
    "#                        lit(\"run_id\"), lit(run_id),\n",
    "#                        lit(\"task_id\"), lit(task_id),\n",
    "#                        lit(\"processed_timestamp\"), lit(processed_timestamp),\n",
    "#                    ))\n",
    "\n",
    "# # df.show(5)\n",
    "# rows_processed = airlines_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0c275e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 3: Write\n",
    "\n",
    "try:\n",
    "    # Count rows first\n",
    "    rows_processed = airlines_df.count()\n",
    "\n",
    "    # Overwrite Delta table safely\n",
    "    airlines_df.write.\\\n",
    "    mode(\"overwrite\").\\\n",
    "    option(\"overwriteSchema\", \"true\").\\\n",
    "    saveAsTable(f\"`{catalog}`.`01_bronze`.`unikargo_airlines_bronze`\")\n",
    "\n",
    "    # Log SUCCESS\n",
    "    log_task_event(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Airlines data written successfully\",\n",
    "        pipeline_name=pipeline_name\n",
    "    )\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    # Log FAILURE safely\n",
    "    try:\n",
    "        log_task_event(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e)\n",
    "        )\n",
    "    except AnalysisException as log_e:\n",
    "        print(f\"Failed to log task event: {log_e}\")\n",
    "    raise  # re-raise original error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f872fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 3: Write\n",
    "\n",
    "\n",
    "# airlines_df.write.\\\n",
    "# mode(\"overwrite\").\\\n",
    "# option(\"overwriteSchema\", \"true\").\\\n",
    "# saveAsTable(f\"`{catalog}`.`01_bronze`.`unikargo_airlines_bronze`\") \n",
    "\n",
    "\n",
    "# log_task_event(pipeline_id, run_id, \"write_bronze\", \"SUCCESS\", rows=df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0effc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_time = datetime.now()\n",
    "# log_pipeline_event(\n",
    "#     pipeline_name=pipeline_name,\n",
    "#     environment=environment,\n",
    "#     run_id=run_id,\n",
    "#     status=status,\n",
    "#     start_time=start_time,\n",
    "#     end_time=end_time,\n",
    "#     rows_processed=rows_processed,\n",
    "#     message=message\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4f3ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "|pipeline_id                         |pipeline_name       |environment |run_id               |task_id    |status |rows|message                           |timestamp                 |\n",
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "|c203e7fc-0ec1-4043-a1ad-1ed8caebb5a8|airlines_ingestion  |unikargo_dev|manual_run_1756729898|manual_task|SUCCESS|14  |Airlines data written successfully|2025-09-01 12:31:38.926468|\n",
      "|1fe5a411-3ece-4011-bc5f-17b37db9d50a|airlines_ingestion  |unikargo_dev|manual_run_1756729894|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:31:35.29408 |\n",
      "|7c95a047-5673-42e1-95f3-f3eb01fbaab7|airlines_ingestion  |unikargo_dev|manual_run_1756729892|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:31:32.996132|\n",
      "|dafb2265-e81a-49ee-a42e-412d67fcbe71|manual_pipeline_name|unikargo_dev|manual_run_1756729773|manual_task|SUCCESS|14  |Airlines data written successfully|2025-09-01 12:29:34.483091|\n",
      "|ffcb778e-d60e-4b72-a6cb-9bb7c5683208|manual_pipeline_name|unikargo_dev|manual_run_1756729770|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:29:30.798302|\n",
      "|f073dc07-9556-4fda-b7ce-8b30cf1499f8|airlines_ingestion  |unikargo_dev|manual_run_1756729767|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:29:28.170499|\n",
      "|4293daf6-c637-479a-a252-4ea5f2038f09|manual_pipeline_name|unikargo_dev|manual_run_1756729720|manual_task|SUCCESS|14  |Airlines data written successfully|2025-09-01 12:28:41.695016|\n",
      "|4c943969-71eb-4bcd-8a5f-299c40d45957|manual_pipeline_name|unikargo_dev|manual_run_1756729562|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:26:03.405529|\n",
      "|e394f5e1-560e-4ba5-a2ab-da08553e4da9|airlines_ingestion  |unikargo_dev|manual_run_1756729559|manual_task|SUCCESS|14  |Airlines data read successfully   |2025-09-01 12:26:00.392464|\n",
      "|20ec3e4f-8438-4dbe-ae89-eece3d65046a|manual_pipeline_name|unikargo_dev|manual_run_1756729423|manual_task|SUCCESS|14  |Metadata column added successfully|2025-09-01 12:23:43.839565|\n",
      "+------------------------------------+--------------------+------------+---------------------+-----------+-------+----+----------------------------------+--------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "LOG_PATH_TASK = \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/task_logs\"\n",
    "\n",
    "try:\n",
    "    logs_df = spark.read.format(\"delta\").load(LOG_PATH_TASK)\n",
    "    logs_df.orderBy(col(\"timestamp\").desc()).show(10, truncate=False)\n",
    "except Exception:\n",
    "    print(f\"No task logs found yet at {LOG_PATH_TASK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6c29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dbc (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
