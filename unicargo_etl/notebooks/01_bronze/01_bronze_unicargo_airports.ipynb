{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae805c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import col\n",
    "# from pyspark.sql.functions import create_map, lit\n",
    "\n",
    "\n",
    "# Current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Go up 3 levels and append 'src'\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..', 'src'))\n",
    "# Add src to sys.path\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from logging_utils.logger import log_task_status\n",
    "from unikargo_utils import add_pipeline_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdd1e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added for adf\n",
    "\n",
    "dbutils.widgets.text(\"pipeline_id\", \"\")\n",
    "dbutils.widgets.text(\"run_id\", \"\")\n",
    "dbutils.widgets.text(\"task_id\", \"\")\n",
    "dbutils.widgets.text(\"processed_timestamp\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"unikargo_dev\")\n",
    "\n",
    "pipeline_id = dbutils.widgets.get(\"pipeline_id\")\n",
    "run_id = dbutils.widgets.get(\"run_id\")\n",
    "task_id = dbutils.widgets.get(\"task_id\")\n",
    "processed_timestamp = dbutils.widgets.get(\"processed_timestamp\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "\n",
    "\n",
    "# Logging parameters for run context\n",
    "pipeline_name = \"airports_ingestion\"\n",
    "environment = \"dev\"\n",
    "# run_id = str(uuid.uuid4())\n",
    "start_time = datetime.now()\n",
    "status = \"SUCCESS\"\n",
    "message = \"\"\n",
    "rows_processed = 0\n",
    "\n",
    "# Path to save logging for tasks\n",
    "LOG_PATH_TASK = \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/task_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d1e09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_schema = StructType([\n",
    "    StructField(\"iata_code\", StringType(), True),\n",
    "    StructField(\"airline\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8aaa1a",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SHOW VOLUMES;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f02ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Read\n",
    "try:\n",
    "    airports_df = (spark.read\n",
    "      .schema(airports_schema)\n",
    "    .option(\"header\", \"true\") \n",
    "      # .csv(f\"/Volumes/{catalog}/00_raw/source_unicargo_data/airports.csv\") # adf cant read from external unity catalog volumes. \n",
    "      .csv(\"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airports.csv\")  # workaround added for adf\n",
    "      )\n",
    "    # Your registered volume is 806d999a-a9fd-4bef-aa04-f1ee2b077888, \n",
    "    # mapped to abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes\n",
    "    \n",
    "    rows_processed = airports_df.count()\n",
    "\n",
    "    # Log SUCCESS\n",
    "    log_task_status(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Airports data read successfully\",\n",
    "        pipeline_name=pipeline_name\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    # --- Log FAILURE\n",
    "    try:\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e)\n",
    "        )\n",
    "    except AnalysisException as log_e:\n",
    "        print(f\"Failed to log task event: {log_e}\")\n",
    "    raise  # re-raise original error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn(\"metadata\",\n",
    "#                    create_map(\n",
    "#                        lit(\"pipeline_id\"), lit(pipeline_id),\n",
    "#                        lit(\"run_id\"), lit(run_id),\n",
    "#                        lit(\"task_id\"), lit(task_id),\n",
    "#                        lit(\"processed_timestamp\"), lit(processed_timestamp),\n",
    "#                    ))\n",
    "# --- Task 2: Add metadata to the dataframe (Pipeline identifier, Run identifier and Task identifier)\n",
    "\n",
    "try:\n",
    "    airports_df = add_pipeline_metadata(airports_df, pipeline_id, run_id, task_id)\n",
    "\n",
    "    # Count rows after transformation\n",
    "    rows_processed = airports_df.count()\n",
    "\n",
    "    # Log SUCCESS\n",
    "    log_task_status(\n",
    "    status=\"SUCCESS\",\n",
    "    rows=rows_processed,\n",
    "    message=\"Metadata column added successfully\",\n",
    "    pipeline_name=pipeline_name\n",
    "\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    # Log FAILURE\n",
    "    log_task_status(\n",
    "        status=\"FAILED\",\n",
    "        message=str(e),\n",
    "        pipeline_name=pipeline_id,\n",
    "  \n",
    "    )\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 3: Write to bronze\n",
    "\n",
    "try:\n",
    "    # Count rows first\n",
    "    rows_processed = airports_df.count()\n",
    "\n",
    "    airports_df.write\\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"overwriteSchema\", \"true\")\\\n",
    "        .saveAsTable(f\"`{catalog}`.`01_bronze`.`unikargo_airports_bronze`\")\n",
    "    \n",
    "        # Log SUCCESS\n",
    "    log_task_status(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Airports data written successfully\",\n",
    "        pipeline_name=pipeline_name\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    # Log FAILURE safely\n",
    "    try:\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e)\n",
    "        )\n",
    "    except AnalysisException as log_e:\n",
    "        print(f\"Failed to log task event: {log_e}\")\n",
    "    raise  # re-raise original error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "779845c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------+------------+--------------------+----------+-------+----+----------------------------------+--------------------------+\n",
      "|pipeline_id                         |pipeline_name     |environment |run_id              |task_id   |status |rows|message                           |timestamp                 |\n",
      "+------------------------------------+------------------+------------+--------------------+----------+-------+----+----------------------------------+--------------------------+\n",
      "|765f651e-3443-41e7-98a4-d40089f91d6a|airports_ingestion|unikargo_dev|local_run_1757018002|local_test|SUCCESS|322 |Airports data written successfully|2025-09-04 20:33:22.877883|\n",
      "|57fb6ee3-b388-46bf-b5ac-041d706d9785|airports_ingestion|unikargo_dev|local_run_1757017990|local_test|SUCCESS|322 |Metadata column added successfully|2025-09-04 20:33:11.043569|\n",
      "|fa409e29-3240-4cca-9db7-7aecaaf1e3f5|airports_ingestion|unikargo_dev|local_run_1757017982|local_test|SUCCESS|322 |Metadata column added successfully|2025-09-04 20:33:03.239074|\n",
      "|e4f608fb-6776-4cc7-a565-6b325a2e02db|airports_ingestion|unikargo_dev|local_run_1757017978|local_test|SUCCESS|322 |Airports data read successfully   |2025-09-04 20:32:59.401987|\n",
      "|a0b8e143-2857-4877-ae17-f90ea6815497|airports_ingestion|unikargo_dev|local_run_1757017923|local_test|SUCCESS|322 |Airlines data written successfully|2025-09-04 20:32:04.578427|\n",
      "|8114824a-adaf-4058-9810-c62e4d757f2b|airports_ingestion|unikargo_dev|local_run_1757017914|local_test|SUCCESS|322 |Metadata column added successfully|2025-09-04 20:31:55.479985|\n",
      "|85afac4d-fe38-4899-9b4b-d027e6198c61|airports_ingestion|unikargo_dev|local_run_1757017907|local_test|SUCCESS|322 |Airlines data read successfully   |2025-09-04 20:31:48.645783|\n",
      "|50ec81ee-59d7-4704-9621-97d1c0854a88|airlines_ingestion|unikargo_dev|local_run_1757017829|local_test|SUCCESS|14  |Airlines data written successfully|2025-09-04 20:30:29.725071|\n",
      "|e4864750-d5a6-4989-b27d-c014df83bb60|airlines_ingestion|unikargo_dev|local_run_1757017817|local_test|SUCCESS|14  |Metadata column added successfully|2025-09-04 20:30:18.88985 |\n",
      "|8c324e7b-8787-41bc-af6b-74e828931561|airlines_ingestion|unikargo_dev|local_run_1757017804|local_test|SUCCESS|14  |Airlines data read successfully   |2025-09-04 20:30:11.161598|\n",
      "+------------------------------------+------------------+------------+--------------------+----------+-------+----+----------------------------------+--------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#  Log read task for debugging\n",
    "try:\n",
    "    logs_df = spark.read.format(\"delta\").load(LOG_PATH_TASK)\n",
    "    logs_df.orderBy(col(\"timestamp\").desc()).show(10, truncate=False)\n",
    "except Exception:\n",
    "    print(f\"No task logs found yet at {LOG_PATH_TASK}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dbc (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
