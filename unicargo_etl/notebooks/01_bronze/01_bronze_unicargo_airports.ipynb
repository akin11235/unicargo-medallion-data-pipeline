{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae805c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "current_dir = os.getcwd() # Current working directory\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..', 'src')) # Go up 3 levels and append 'src'\n",
    "sys.path.append(project_root) # Add src to sys.path\n",
    "from unikargo_utils import add_pipeline_metadata\n",
    "from config import get_log_adls_path, get_table_config\n",
    "from logging_utils import TaskLogger\n",
    "from io_utils import _get_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd1e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableConfig(catalog='unikargo_dev', schema='01_bronze', table='unikargo_airport_bronze', layer='bronze', table_key=None, format='delta', raw_path='abfss://medallion@adlsunikargodev.dfs.core.windows.net/raw/volumes/airports.csv')\n"
     ]
    }
   ],
   "source": [
    "# Create widgets (required for ADF â†’ Databricks integration)\n",
    "# dbutils.widgets.text(\"pipeline_id\", \"\")\n",
    "# dbutils.widgets.text(\"run_id\", \"\")\n",
    "# dbutils.widgets.text(\"task_id\", \"\")\n",
    "# dbutils.widgets.text(\"processed_timestamp\", \"\")\n",
    "# dbutils.widgets.text(\"catalog\", \"unikargo_dev\")\n",
    "# dbutils.widgets.text(\"ENV\", \"dev\") \n",
    "\n",
    "# Extract values from widgets\n",
    "# pipeline_id = dbutils.widgets.get(\"pipeline_id\")\n",
    "# run_id = dbutils.widgets.get(\"run_id\")\n",
    "# task_id = dbutils.widgets.get(\"task_id\")\n",
    "# processed_timestamp = dbutils.widgets.get(\"processed_timestamp\")\n",
    "# catalog = dbutils.widgets.get(\"catalog\")\n",
    "# ENV = dbutils.widgets.get(\"ENV\")  # -> \"dev\". From the variables set in databricks.yml and unikargo_etl_pipeline_nb.job.yml\n",
    "\n",
    "# Logging parameters for run context\n",
    "pipeline_name = \"pl_unikargo_medallion\"\n",
    "rows_processed = 0\n",
    "log_type =  'task'\n",
    "# environment = \"dev\"\n",
    "environment = _get_widget(\"ENV\", \"dev\")\n",
    "entity=\"airports\"\n",
    "layer=\"bronze\"\n",
    "\n",
    "# .csv(f\"/Volumes/{catalog}/00_raw/source_unicargo_data/airports.csv\") # adf cant read from external unity catalog volumes. \n",
    "# Your registered volume is 806d999a-a9fd-4bef-aa04-f1ee2b077888, \n",
    "# mapped to abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes\n",
    "airports_cfg = get_table_config(entity=\"airports\", layer=\"bronze\", environment=environment)\n",
    "print(airports_cfg)\n",
    "# source_path=\"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/airports.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1e09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_schema = StructType([\n",
    "    StructField(\"iata_code\", StringType(), True),\n",
    "    StructField(\"airline\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f02ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Read\n",
    "airports_csv_path = airports_cfg.raw_path\n",
    "operation = \"tsk_flights_read_raw\"\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    pipeline_name=pipeline_name,\n",
    "    source_path=airports_csv_path,\n",
    "    log_running=False \n",
    "\n",
    ") as logger:\n",
    "    \n",
    "    airports_df = (spark.read\n",
    "      .schema(airports_schema)\n",
    "      .option(\"header\", \"true\") \n",
    "      .csv(airports_csv_path) \n",
    "      )\n",
    "    \n",
    "    \n",
    "    rows_processed = airports_df.count()\n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8405b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# --- Task 2: Add metadata to the dataframe (Pipeline identifier, Run identifier and Task identifier)\n",
    "# -----------------------------\n",
    "operation=\"tsk_flights_add_metadata\"\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    pipeline_name=pipeline_name,\n",
    "    log_running=False \n",
    ") as logger:\n",
    "\n",
    "    # airports_df = add_pipeline_metadata(airports_df, pipeline_id, run_id, task_id)\n",
    "\n",
    "    airports_df = add_pipeline_metadata(\n",
    "    airports_df,\n",
    "    pipeline_id=logger.kwargs.get(\"pipeline_id\"),\n",
    "    run_id=logger.kwargs.get(\"run_id\"),\n",
    "    task_id=logger.kwargs.get(\"task_id\")\n",
    ")\n",
    "\n",
    "    # Count rows after transformation\n",
    "    rows_processed = airports_df.count()\n",
    "\n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0558f7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unikargo_dev.01_bronze.unikargo_airport_bronze\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Write to bronze\n",
    "# -----------------------------\n",
    "target_path = airports_cfg.full_name\n",
    "print(target_path)\n",
    "operation = \"tsk_airports_persist_bronze\"\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    pipeline_name=pipeline_name,\n",
    "    target_path=target_path,\n",
    "    log_running=False\n",
    ") as logger:\n",
    "    \n",
    "    # Count rows first\n",
    "    rows_processed = airports_df.count()\n",
    "\n",
    "    airports_df.write\\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"overwriteSchema\", \"true\")\\\n",
    "        .saveAsTable(target_path)\n",
    "    \n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------Uncomment to debug (Read Delta logs and show latest logs)-----------------\n",
    "\n",
    "log_path = get_log_adls_path(log_type, environment=environment)\n",
    "logs_df = spark.read.format(\"delta\").load(log_path)\n",
    "logs_df.orderBy(\"timestamp\", ascending=False).show(20, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dbc (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
