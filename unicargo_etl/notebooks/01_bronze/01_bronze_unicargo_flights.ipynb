{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Go up 3 levels and append 'src'\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..', 'src'))\n",
    "# Add src to sys.path\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from logging_utils.logger import log_task_status\n",
    "from unikargo_utils import add_pipeline_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bfac6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dbutils.widgets.text(\"pipeline_id\", \"\")\n",
    "dbutils.widgets.text(\"run_id\", \"\")\n",
    "dbutils.widgets.text(\"task_id\", \"\")\n",
    "dbutils.widgets.text(\"processed_timestamp\", \"\")\n",
    "dbutils.widgets.text(\"catalog\", \"unikargo_dev\")\n",
    "\n",
    "pipeline_id = dbutils.widgets.get(\"pipeline_id\")\n",
    "run_id = dbutils.widgets.get(\"run_id\")\n",
    "task_id = dbutils.widgets.get(\"task_id\")\n",
    "processed_timestamp = dbutils.widgets.get(\"processed_timestamp\")\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "\n",
    "# Logging parameters for run context\n",
    "pipeline_name = \"flights_ingestion\"\n",
    "environment = \"dev\"\n",
    "# run_id = str(uuid.uuid4())\n",
    "start_time = datetime.now()\n",
    "status = \"SUCCESS\"\n",
    "message = \"\"\n",
    "rows_processed = 0\n",
    "\n",
    "# Path to save logging for tasks\n",
    "LOG_PATH_TASK = \"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/logs/task_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9ff25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"day_of_week\", IntegerType(), True),\n",
    "    StructField(\"airline\", StringType(), True),\n",
    "    StructField(\"flight_number\", IntegerType(), True),\n",
    "    StructField(\"tail_number\", StringType(), True),\n",
    "    StructField(\"origin_airport\", StringType(), True),\n",
    "    StructField(\"destination_airport\", StringType(), True),\n",
    "    StructField(\"scheduled_departure\", IntegerType(), True),\n",
    "    StructField(\"departure_time\", IntegerType(), True),\n",
    "    StructField(\"departure_delay\", IntegerType(), True),\n",
    "    StructField(\"taxi_out\", IntegerType(), True),\n",
    "    StructField(\"wheels_off\", IntegerType(), True),\n",
    "    StructField(\"scheduled_time\", IntegerType(), True),\n",
    "    StructField(\"elapsed_time\", IntegerType(), True),\n",
    "    StructField(\"air_time\", IntegerType(), True),\n",
    "    StructField(\"distance\", IntegerType(), True),\n",
    "    StructField(\"wheels_on\", IntegerType(), True),\n",
    "    StructField(\"taxi_in\", IntegerType(), True),\n",
    "    StructField(\"scheduled_arrival\", IntegerType(), True),\n",
    "    StructField(\"arrival_time\", IntegerType(), True),\n",
    "    StructField(\"arrival_delay\", IntegerType(), True),\n",
    "    StructField(\"diverted\", IntegerType(), True),\n",
    "    StructField(\"cancelled\", IntegerType(), True),\n",
    "    StructField(\"cancellation_reason\", StringType(), True),\n",
    "    StructField(\"air_system_delay\", IntegerType(), True),\n",
    "    StructField(\"security_delay\", IntegerType(), True),\n",
    "    StructField(\"airline_delay\", IntegerType(), True),\n",
    "    StructField(\"late_aircraft_delay\", IntegerType(), True),\n",
    "    StructField(\"weather_delay\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    flights_df = (spark.read\n",
    "        .schema(flights_schema)\n",
    "        .option(\"header\", \"true\") \n",
    "        # .csv(f\"/Volumes/{catalog}/00_raw/source_unicargo_data/flights.csv\")\n",
    "        .csv(\"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/flights.csv\") # added for adf\n",
    ")\n",
    "    \n",
    "    rows_processed = flights_df.count()\n",
    "    \n",
    "    # Log SUCCESS\n",
    "    log_task_status(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Flights data read successfully\",\n",
    "        pipeline_name=pipeline_name\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    # --- Log FAILURE\n",
    "    try:\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e)\n",
    "        )\n",
    "    except AnalysisException as log_e:\n",
    "        print(f\"Failed to log task event: {log_e}\")\n",
    "    raise  # re-raise original error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8091b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    flights_df_df = add_pipeline_metadata(flights_df, pipeline_id, run_id, task_id)\n",
    "\n",
    "    # Count rows after transformation\n",
    "    rows_processed = flights_df.count()\n",
    "\n",
    "    # Log SUCCESS\n",
    "    log_task_status(\n",
    "    status=\"SUCCESS\",\n",
    "    rows=rows_processed,\n",
    "    message=\"Metadata column added successfully\",\n",
    "    pipeline_name=pipeline_name\n",
    "\n",
    "    )\n",
    "except Exception as e:\n",
    "    # Log FAILURE\n",
    "    log_task_status(\n",
    "        status=\"FAILED\",\n",
    "        message=str(e),\n",
    "        pipeline_name=pipeline_id,\n",
    "  \n",
    "    )\n",
    "    raise\n",
    "\n",
    "\n",
    "# df = df.withColumn(\"metadata\",\n",
    "#                    create_map(\n",
    "#                        lit(\"pipeline_id\"), lit(pipeline_id),\n",
    "#                        lit(\"run_id\"), lit(run_id),\n",
    "#                        lit(\"task_id\"), lit(task_id),\n",
    "#                        lit(\"processed_timestamp\"), lit(processed_timestamp),\n",
    "#                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877642f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Count rows first\n",
    "    rows_processed = flights_df.count()\n",
    "\n",
    "    flights_df.write.\\\n",
    "    mode(\"overwrite\").\\\n",
    "    option(\"overwriteSchema\", \"true\").\\\n",
    "    saveAsTable(f\"`{catalog}`.`01_bronze`.`unikargo_flights_bronze`\")\n",
    "\n",
    "        # Log SUCCESS\n",
    "    log_task_status(\n",
    "        status=\"SUCCESS\",\n",
    "        rows=rows_processed,\n",
    "        message=\"Flights data written successfully\",\n",
    "        pipeline_name=pipeline_name\n",
    "    )\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "        # Log FAILURE safely\n",
    "    try:\n",
    "        log_task_status(\n",
    "            status=\"FAILED\",\n",
    "            message=str(e)\n",
    "        )\n",
    "    except AnalysisException as log_e:\n",
    "        print(f\"Failed to log task event: {log_e}\")\n",
    "    raise  # re-raise original error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5338616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------+------------+--------------------+----------+-------+-------+----------------------------------+--------------------------+\n",
      "|pipeline_id                         |pipeline_name     |environment |run_id              |task_id   |status |rows   |message                           |timestamp                 |\n",
      "+------------------------------------+------------------+------------+--------------------+----------+-------+-------+----------------------------------+--------------------------+\n",
      "|859730bb-7fa0-4876-909f-91701faadfde|flights_ingestion |unikargo_dev|local_run_1757018668|local_test|SUCCESS|5819079|Flights data written successfully |2025-09-04 20:44:28.772976|\n",
      "|04d5e861-639b-4970-9fcc-d1ed92f234b2|flights_ingestion |unikargo_dev|local_run_1757018634|local_test|SUCCESS|5819079|Metadata column added successfully|2025-09-04 20:43:54.751967|\n",
      "|dea48e03-b9d6-49ed-9d96-c23b825748d2|flights_ingestion |unikargo_dev|local_run_1757018629|local_test|SUCCESS|5819079|Flights data read successfully    |2025-09-04 20:43:50.099575|\n",
      "|a1851a6a-c9ba-4929-91d6-4a0ae2a82ea9|flights_ingestion |unikargo_dev|local_run_1757018572|local_test|SUCCESS|5819079|Airlines data written successfully|2025-09-04 20:42:53.48887 |\n",
      "|2882036d-364b-43f9-97ca-d176837192ad|flights_ingestion |unikargo_dev|local_run_1757018532|local_test|SUCCESS|5819079|Metadata column added successfully|2025-09-04 20:42:13.082493|\n",
      "|5daeb9d5-2db2-4082-80c4-0e208b74d183|flights_ingestion |unikargo_dev|local_run_1757018527|local_test|SUCCESS|5819079|Flights data read successfully    |2025-09-04 20:42:08.238056|\n",
      "|21f26606-7fc5-4e96-b358-1b94407d9e25|airlines_ingestion|unikargo_dev|local_run_1757018522|local_test|SUCCESS|5819079|Airlines data written successfully|2025-09-04 20:42:03.552171|\n",
      "|708f4072-c18d-4b90-a28d-69c0c14b48af|airlines_ingestion|unikargo_dev|local_run_1757018442|local_test|SUCCESS|5819079|Metadata column added successfully|2025-09-04 20:40:43.273037|\n",
      "|d756008f-686e-46c9-8633-edf12e6e201c|airlines_ingestion|unikargo_dev|local_run_1757018437|local_test|SUCCESS|5819079|Flights data read successfully    |2025-09-04 20:40:38.403291|\n",
      "|648c233f-b149-48f0-a121-248363915d74|airlines_ingestion|unikargo_dev|local_run_1757018375|local_test|SUCCESS|5819079|Metadata column added successfully|2025-09-04 20:39:35.869263|\n",
      "+------------------------------------+------------------+------------+--------------------+----------+-------+-------+----------------------------------+--------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#  Log read task for debugging\n",
    "try:\n",
    "    logs_df = spark.read.format(\"delta\").load(LOG_PATH_TASK)\n",
    "    logs_df.orderBy(col(\"timestamp\").desc()).show(10, truncate=False)\n",
    "except Exception:\n",
    "    print(f\"No task logs found yet at {LOG_PATH_TASK}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dbc (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
