{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "current_dir = os.getcwd() # Current working directory\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..', 'src')) # Go up 3 levels and append 'src'\n",
    "sys.path.append(project_root) # Add src to sys.path\n",
    "\n",
    "from logging_utils import TaskLogger\n",
    "from unikargo_utils import add_pipeline_metadata\n",
    "from config import get_log_adls_path, get_table_config\n",
    "from io_utils import _get_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfac6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create widgets (required for ADF â†’ Databricks integration)\n",
    "# Dynamic value references for job tasks\n",
    "# Retrieve base parameters\n",
    "# dbutils.widgets.text(\"pipeline_id\", \"\")\n",
    "# dbutils.widgets.text(\"run_id\", \"\")\n",
    "# dbutils.widgets.text(\"task_id\", \"\")\n",
    "# dbutils.widgets.text(\"processed_timestamp\", \"\")\n",
    "# # dbutils.widgets.text(\"catalog\", \"unikargo_dev\")\n",
    "# dbutils.widgets.text(\"ENV\", \"dev\") \n",
    "\n",
    "# # # Extract values from widgets\n",
    "# pipeline_id = dbutils.widgets.get(\"pipeline_id\")\n",
    "# run_id = dbutils.widgets.get(\"run_id\")\n",
    "# task_id = dbutils.widgets.get(\"task_id\")\n",
    "# processed_timestamp = dbutils.widgets.get(\"processed_timestamp\")\n",
    "# # catalog = dbutils.widgets.get(\"catalog\")\n",
    "# ENV = dbutils.widgets.get(\"ENV\")  # -> \"dev\". From the variables set in databricks.yml and unikargo_etl_pipeline_nb.job.yml\n",
    "\n",
    "\n",
    "# Logging parameters for run context\n",
    "# pipeline_name = \"pl_unikargo_medallion\"\n",
    "# pipeline_name = _get_widget(\"pipeline_name\", \"pl_unikargo_medallion\")\n",
    "\n",
    "\n",
    "rows_processed = 0\n",
    "log_type =  'task'\n",
    "# environment = 'dev'\n",
    "# environment = logger.kwargs.get(\"environment\", \"dev\")\n",
    "environment = _get_widget(\"ENV\", \"dev\")\n",
    "run_id = _get_widget(\"run_id\")\n",
    "# run_name = _get_widget(\"run_name\")\n",
    "\n",
    "entity=\"airports\"\n",
    "layer=\"bronze\"\n",
    "# .csv(f\"/Volumes/{catalog}/00_raw/source_unicargo_data/flights.csv\")\n",
    "# flights_cfg = get_table_config(entity=\"flights\", layer=\"bronze\", environment=\"dev\")\n",
    "flights_cfg = get_table_config(entity=\"flights\", layer=\"bronze\", environment=environment)\n",
    "print(flights_cfg)\n",
    "# source_path=\"abfss://medallion@adlsunikarrgodev.dfs.core.windows.net/raw/volumes/flights.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"day_of_week\", IntegerType(), True),\n",
    "    StructField(\"airline\", StringType(), True),\n",
    "    StructField(\"flight_number\", IntegerType(), True),\n",
    "    StructField(\"tail_number\", StringType(), True),\n",
    "    StructField(\"origin_airport\", StringType(), True),\n",
    "    StructField(\"destination_airport\", StringType(), True),\n",
    "    StructField(\"scheduled_departure\", IntegerType(), True),\n",
    "    StructField(\"departure_time\", IntegerType(), True),\n",
    "    StructField(\"departure_delay\", IntegerType(), True),\n",
    "    StructField(\"taxi_out\", IntegerType(), True),\n",
    "    StructField(\"wheels_off\", IntegerType(), True),\n",
    "    StructField(\"scheduled_time\", IntegerType(), True),\n",
    "    StructField(\"elapsed_time\", IntegerType(), True),\n",
    "    StructField(\"air_time\", IntegerType(), True),\n",
    "    StructField(\"distance\", IntegerType(), True),\n",
    "    StructField(\"wheels_on\", IntegerType(), True),\n",
    "    StructField(\"taxi_in\", IntegerType(), True),\n",
    "    StructField(\"scheduled_arrival\", IntegerType(), True),\n",
    "    StructField(\"arrival_time\", IntegerType(), True),\n",
    "    StructField(\"arrival_delay\", IntegerType(), True),\n",
    "    StructField(\"diverted\", IntegerType(), True),\n",
    "    StructField(\"cancelled\", IntegerType(), True),\n",
    "    StructField(\"cancellation_reason\", StringType(), True),\n",
    "    StructField(\"air_system_delay\", IntegerType(), True),\n",
    "    StructField(\"security_delay\", IntegerType(), True),\n",
    "    StructField(\"airline_delay\", IntegerType(), True),\n",
    "    StructField(\"late_aircraft_delay\", IntegerType(), True),\n",
    "    StructField(\"weather_delay\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3025fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flights_cfg.raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS unikargo_dev.01_bronze.unikargo_flights_bronze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Read the flights CSV\n",
    "# -----------------------------\n",
    "flights_csv_path = flights_cfg.raw_path\n",
    "operation = \"tsk_flights_read_raw\"\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    # pipeline_name=pipeline_name,\n",
    "    source_path=flights_csv_path,\n",
    "    log_running=False  # keep this False unless you explicitly want a \"RUNNING\" entry\n",
    ") as logger:\n",
    "\n",
    "    flights_df = (spark.read\n",
    "        .schema(flights_schema)\n",
    "        .option(\"header\", \"true\") \n",
    "        .csv(flights_csv_path) \n",
    ")\n",
    "    \n",
    "    rows_processed = flights_df.count()\n",
    "        \n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8091b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# --- Task 2: Add metadata to the dataframe \n",
    "# -----------------------------\n",
    "operation=\"tsk_flights_add_metadata\"\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    # pipeline_name=pipeline_name,\n",
    "    log_running=False \n",
    ") as logger:\n",
    "\n",
    "    # flights_df_df = add_pipeline_metadata(flights_df, pipeline_id, run_id, task_id)\n",
    "\n",
    "    flights_df = add_pipeline_metadata(\n",
    "    flights_df,\n",
    "    pipeline_id=logger.kwargs.get(\"pipeline_id\"),\n",
    "    run_id=logger.kwargs.get(\"run_id\"),\n",
    "    task_id=logger.kwargs.get(\"task_id\")\n",
    ")\n",
    "\n",
    "    # Count rows after transformation\n",
    "    rows_processed = flights_df.count()\n",
    "\n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877642f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Write to bronze\n",
    "# -----------------------------\n",
    "target_path = flights_cfg.full_name\n",
    "print(\"target_path: \", target_path)\n",
    "operation = \"tsk_flights_persist_bronze\"\n",
    "\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    # pipeline_name=pipeline_name,\n",
    "    target_path=target_path,\n",
    "    log_running=False\n",
    ") as logger:\n",
    "\n",
    "    # Count rows first\n",
    "    rows_processed = flights_df.count()\n",
    "\n",
    "    flights_df.write.\\\n",
    "    mode(\"overwrite\").\\\n",
    "    option(\"overwriteSchema\", \"true\").\\\n",
    "    saveAsTable(target_path)\n",
    "\n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7874e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_csv_path = flights_cfg.raw_path\n",
    "operation = \"tsk_flights_read_raw_error\"\n",
    "\n",
    "try:\n",
    "    with TaskLogger(\n",
    "        operation=operation,\n",
    "        # pipeline_name=pipeline_name,\n",
    "        source_path=flights_csv_path,\n",
    "        log_running=False\n",
    "    ) as logger:\n",
    "\n",
    "        # Simulate reading flights data\n",
    "        flights_df = (spark.read\n",
    "            .schema(flights_schema)\n",
    "            .option(\"header\", \"true\")\n",
    "            .csv(flights_csv_path)\n",
    "        )\n",
    "\n",
    "        # Artificially trigger an error\n",
    "        raise ValueError(\"Simulated failure for testing TaskLogger\")\n",
    "\n",
    "        rows_processed = flights_df.count()\n",
    "        logger.set_metrics(rows=rows_processed)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Caught error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------Uncomment to debug (Read Delta logs and show latest logs)-----------------\n",
    "\n",
    "log_path = get_log_adls_path(log_type, environment=environment) # Path to save logging for tasks\n",
    "logs_df = spark.read.format(\"delta\").load(log_path)\n",
    "logs_df.orderBy(\"timestamp\", ascending=False).show(20, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dbc (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
