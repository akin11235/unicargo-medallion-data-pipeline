{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff493e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks Notebook: silver_processing.py\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "current_dir = os.getcwd() # Current working directory\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..', 'src')) # Go up 3 levels and append 'src'\n",
    "sys.path.append(project_root) # Add src to sys.path\n",
    "\n",
    "from logging_utils import TaskLogger\n",
    "from config import get_log_adls_path, get_table_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"pl_unikargo_medallion\"\n",
    "log_type =  'task'\n",
    "entity=\"airlines\"\n",
    "environment = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e69967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1aab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# === STEP 1: READ AIRLINES DATA ===\n",
    "# -----------------------------\n",
    "source_layer=\"bronze\"\n",
    "airlines_cfg = get_table_config(entity=entity, layer=source_layer, environment=environment)\n",
    "source_path = airlines_cfg.full_name\n",
    "# source_path = \"unikargo_dev.01_bronze.unikargo_airlines_bronze\"\n",
    "operation = \"tsk_airlines_read_bronze\"\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    pipeline_name=pipeline_name,\n",
    "    source_path=source_path,\n",
    "    log_running=False  # keep this False unless you explicitly want a \"RUNNING\" entry\n",
    ")as logger:\n",
    "\n",
    "    airlines_df = spark.read.table(source_path)\n",
    "\n",
    "    rows_processed = airlines_df.count()\n",
    "    \n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# --- Task 2: Deduplicate & Generate Airline Dimension\n",
    "# -----------------------------\n",
    "\n",
    "operation=\"tsk_airlines_dim_build\"\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    pipeline_name=pipeline_name,\n",
    "    log_running=False \n",
    ") as logger:\n",
    "    \n",
    "    dim_airline = airlines_df \\\n",
    "        .filter(col(\"iata_code\").isNotNull()) \\\n",
    "        .dropDuplicates([\"iata_code\"]) \\\n",
    "        .select(\"iata_code\", \"airline\") \\\n",
    "        .withColumn(\"airline_sk\", row_number().over(Window.orderBy(\"iata_code\"))) \\\n",
    "        .select(\"airline_sk\", \"iata_code\", \"airline\")\n",
    "    \n",
    "    rows_processed = airlines_df.count()\n",
    "    \n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer=\"silver\"\n",
    "airlines_cfg = get_table_config(entity=entity, layer=target_layer, environment=environment)\n",
    "target_path = airlines_cfg.full_name\n",
    "\n",
    "print(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef11616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Write the Delta table\n",
    "# -----------------------------\n",
    "target_layer=\"silver\"\n",
    "airlines_cfg = get_table_config(entity=entity, layer=target_layer, environment=environment)\n",
    "target_path = airlines_cfg.full_name\n",
    "\n",
    "operation = \"tsk_airlines_persist_silver\"\n",
    "# target_path =\"`unikargo_dev`.`02_silver`.`unikargo_dim_airline_silver`\"\n",
    "\n",
    "with TaskLogger(\n",
    "    operation=operation,\n",
    "    pipeline_name=pipeline_name,\n",
    "    target_path = target_path,\n",
    "    log_running=False\n",
    ") as logger:\n",
    "    \n",
    "    # Count rows first\n",
    "    rows_processed = airlines_df.count()\n",
    "\n",
    "    dim_airline.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(target_path)\n",
    "    \n",
    "    # Update metrics before completion\n",
    "    logger.set_metrics(rows=rows_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------Uncomment to debug (Read Delta logs and show latest logs)-----------------\n",
    "log_path = get_log_adls_path(log_type, environment=environment) # Path to save logging for tasks\n",
    "\n",
    "logs_df = spark.read.format(\"delta\").load(log_path)\n",
    "logs_df.orderBy(\"timestamp\", ascending=False).show(20, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dbc (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
